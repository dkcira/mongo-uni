##########################################
# m040 notes
##########################################

##########################################
# copy setup file to m040/shared/
ls -1 m040/shared
rs1_linux.conf
rs1_win.conf
rs2_linux.conf
rs2_win.conf
rs3_linux.conf
rs3_win.conf
validate_lab1.js

# get into vagrant
vagrant ssh

# mongod version
mongod --version

db version v4.0.9
git version: fc525e2d9b0e4bceff5c2201457e564362909765
OpenSSL version: OpenSSL 1.0.1f 6 Jan 2014
allocator: tcmalloc
modules: enterprise 
build environment:
    distmod: ubuntu1404
    distarch: x86_64
    target_arch: x86_64

# replicaset data dirs
mkdir -p /data/db/m040/repl/{1,2,3}

# start up mongod processes
mongod -f /shared/initial_setup/rs1_linux.conf
mongod -f /shared/initial_setup/rs2_linux.conf
mongod -f /shared/initial_setup/rs3_linux.conf

# initiate the replicaset
mongo --eval 'rs.initiate()'
MongoDB server version: 4.0.9
{
        "info2" : "no configuration specified. Using a default configuration for the set",
        "me" : "m040:27017",
        "ok" : 1,
        "operationTime" : Timestamp(1564333649, 1),
        "$clusterTime" : {
                "clusterTime" : Timestamp(1564333649, 1),
                "signature" : {
                        "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
                        "keyId" : NumberLong(0)
                }
        }
}

# add the remaining two nodes to the set
mongo --eval 'rs.add("m040:27027");rs.add("m040:27037")'
mongo --eval 'rs.status()'


# make sure you have setup the cluster correctly
mongo --quiet /shared/initial_setup/validate_lab1.js

Great Job! Your validation code is:
1602589782

 # Quiz
Q: Which of the following are true in the case of multi-document transaction operations?
Attempts Remaining:âˆžUnlimited Attempts

A:
WiredTiger can proactively evict snapshots and abort transactions if cache pressure thresholds are reached.
If a transaction takes more than 60 seconds to complete, it will abort.
In cases where there is a significant amount of write operations in one single transaction (more than 1000 changed documents), the transaction should be broken into smaller sets of operations.

There are no practical limits to the number of documents that can be read in a transaction. There's only the limit of up to 1000 documents modified in a single transaction. For those cases, we should split the transaction across several transactions.

Q:
Which of the following statements are correct?
A:
Reads can still occur while a write lock is taken.
The correct answer is:
Reads can still occur when a write lock is taken.
A write lock does not affect the read availablity of documents. Only writes may be affected, if there is concurrency over the same documents, or if a write lock is taken by a transaction.
All other options are incorrect.

All writes not in a transaction context will be applied before the transaction write lock are released.
This is incorrect, operations wait until the transaction aborts or commits.

All writes not in a transaction context will fail while a transaction is taking place.
This is incorrect, operations wait until the transaction aborts or commits.

Q: Abort vs Commit. Problem: What can cause TransientTransactionError to occur?

A:
Network failures
WriteConflict errors

The cases where TransientTransactionError can occur are related with:
Network failures
Network failures may be related with server-client communication failure or abnormal issues with the network layer.
WriteConflict errors
WriteConflict will be detected when two concurrent transactions try to write to the same document, where one of those transactions acquires the document lock and subsequent requests from other transactions will result in WriteConflict errors raised.

All other options are incorrect.


##########################################
# Lab: Error Handling

Problem:

In this lab we will be exploring error handling related with transaction support.

As for handouts of this lab, you can find two files:

loader.py
data.json
Handouts Description

loader.py is a script that does the following set of steps:

connects to a MongoDB replica set (by default running on this course m040 VM).
mongodb://m040:27017/m040?replicaSet=M040

Drops any previous existing data in the m040.cities and m040.city_stats collections.
Opens the data.json file and process it in batches. Each batch is processed by its own thread/process to simulate concurrent database write load.
Each batch process starts a session.
For each session, a transaction gets initiated:
Writes the batch of city documents into m040.cities collection.
Updates the m040.city_stats total population field.
The data.json is json file that contains one hundred imaginary cities and their (imaginary) population counts.
Within data.json there are some logical errors, like duplicate keys, which were introduced on purpose. Do not change this file.

Setup
In order for the loader.py script to execute you will have to have a running replica set, M040.
Using this course VM, you can set up the replica set by running the following commands:

# setup replicaset
vagrant up
vagrant ssh

mongod --dbpath ~/repl/1 --logpath ~/repl/1/log --port 27017 --replSet M040 --bind_ip_all --fork
mongod --dbpath ~/repl/2 --logpath ~/repl/2/log --port 27027 --replSet M040 --bind_ip_all --fork
mongod --dbpath ~/repl/3 --logpath ~/repl/3/log --port 27037 --replSet M040 --bind_ip_all --fork

mongo --eval 'rs.initiate()'
mongo --eval 'rs.add("192.168.40.100:27027");rs.add("192.168.40.100:27037")'
mongo --eval 'conf = rs.conf();conf.members[0].host="192.168.40.100:27017";rs.reconfig(conf);'

# run the python
cd /shared/lab_transactions/
python3 ./loader.py

Duplicate Key Found: E11000 duplicate key error collection: m040.cities index: _id_ dup key: { : 81 }
Error - what shall I do ??!??! WriteConflict
Error - what shall I do ??!??! WriteConflict
Unexpected error found: WriteConflict
Unexpected error found: WriteConflict
Duplicate Key Found: E11000 duplicate key error collection: m040.cities index: _id_ dup key: { : 90 }
Duplicate Key Found: E11000 duplicate key error collection: m040.cities index: _id_ dup key: { : 41 }
Documents Inserted: 50
Total Population: 901563036
vagrant@m040:/shared/lab_transactions$ 


##########################################
# Server improvements

TLS => Transport Layer Security

PCI DSS => Payment Card Industry Data Security Standard
NIST    => National Institute for Standards and Technology
FIPS    => Federal Information Processing Standard

MongoDB 4.0 support TLS1.2 for all communication between client, drivers, server, and internal cluster communication between nodes. No special upgrade or downgrade procedures are required.

TLS:
- Your data should be securely encrypted in transit to prevent unauthorized access to it via the network.
- TLS certificates can be used as a means of securely confirming the identity of an entity attempting to access your data.
- Without it, you may fail to meet corporate security policies which can in turn prevent your applications from going into production.


# MongoDB 4.0 authentication enhancements
- MongoDB CR authentication is removed and deprecated
- SCRAM-SHA1 was introduced in 2015 (Mongo 3.0) => upgrade to this is needed before upgrading to MongoDB 4.0
- SCRAM-SHA-256 authentication mechanism added. You should only use this, exclusively.
- If you enabled SCRAM-SHA1, automatic upgrade to SCRAM-SHA-256. New users will use it by default. => Authentication mechanism server parameter

- command to list users that do not have scram-sha-256 support yet
db.runCommand(
    {usersInfo:{forAllDBs:true}},
    filter:{
        mechanisms:{$ne:"SCRAM-SHA-256"}
        }
)
- In MongoDB4.0 just resetting the password will add scram-sha-256 support
db.updateUser("oldmatt", {pwd: "pass"})
- The users should change the password, not the DBA, since he should not know their passwords.

# Change Stream
db.collection.watch() <= for collection
db.watch() <= for whole db
also for cluster, regardless of the db 

Quiz
The correct answers are:

Listen for changes in documents across all collections in a database
Correct, in MongoDB 4.0 a change stream can be set to listen to changes across all collections in a database

Listen for changes across a cluster
Correct, this is also something new that MongoDB 4.0 allows.

Change Event document provides information on txnNumber
Correct, if documents have been changed within the execution of a transaction, the Change Event will contain the field txnNumber

All other options are incorrect.

Resume a change stream

This feature has been available since the inception of Change Streams.

Open a change stream to track update operations only

Change streams have always allowed to get change events on operation type including updates.

##########################################
# Aggregation Framework  Improvements

# Lab: Type Conversions
vagrant up
vagrant ssh

mongod --dbpath ~/repl/1 --logpath ~/repl/1/log --port 27017 --replSet M040 --bind_ip_all --fork
mongod --dbpath ~/repl/2 --logpath ~/repl/2/log --port 27027 --replSet M040 --bind_ip_all --fork
mongod --dbpath ~/repl/3 --logpath ~/repl/3/log --port 27037 --replSet M040 --bind_ip_all --fork

mongo --eval 'rs.initiate()'
mongo --eval 'rs.add("192.168.40.100:27027");rs.add("192.168.40.100:27037")'
mongo --eval 'conf = rs.conf();conf.members[0].host="192.168.40.100:27017";rs.reconfig(conf);'

mongoimport --host M040/$(hostname):27017,$(hostname):27027,$(hostname):27037 -d sensor -c data < /shared/lab_typeconversions/sensor_data.json

2019-07-30T04:54:43.169+0000	connected to: M040/m040:27017,m040:27027,m040:27037
2019-07-30T04:54:43.242+0000	imported 31 documents

# initial code
db.data.aggregate([
  {
    "$unwind": "$turnstile"
  },
  {
    "$group": {
      "_id": 0,
      "sum": {
        "$sum": {
        <your code here!!>
        }
      }
    }
  }
])

# my (correct) solution
db.data.aggregate([
  {
    "$unwind": "$turnstile"
  },
  {
    "$group": {
      "_id": 0,
      "sum": {
        "$sum": {
                $convert: {
                        input: "$turnstile.count",
                        to: "int",
                        onError: 0
                }
        }
      }
    }
  }
])

{ "_id" : 0, "sum" : 281245 }

# official solution
db.data.aggregate([
  {
    "$unwind": "$turnstile"
  },
  {
    "$group": {
      "_id": 0,
      "sum": {
        "$sum": {
          "$convert": {
            "input": "$turnstile.count",
            "to": "int",
            "onError": 0,
            "onNull": 0
          }
        }
      }
    }
  }
])

You may have reached the same result with a different pipeline and using a different set of aggregation stages (congratulations!) but the important thing is understanding the data conversion necessary to calculate the sum.

In our solution, we use $unwind to unwind the turnstiles array.

We then group all documents together, and sum the values of the turnstile.count field. If this field is missing it will be handled by the onNull clause, and if this field can't be converted to an int it will be handled by our onError clause.

##########################################
# MongoDB Compass

Quiz
Q: Which of the following is/are new features in Compass?

Aggregation Pipeline Builder.
This is correct.
This feature dramatically decreases the time it takes to write an aggregation pipeline.

Read-Only Compass Edition.
This is correct.
Use this edition for users who only need to browse the documents, and have no need in editing them.

Export Queries into Programming Language Code.
This is correct.
Compass can translate the queries from its graphical user interface to the most popular programming languages like: Java, JavaScript, Python and C#.

Import and Export from/to JSON and CSV files.
This is correct.
This was the most requested feature, and now Compass has it. So, ensure you transmit us your feedback on the tool, because we are listening and we want to hear form you!

Translator of text fields into 100 languages.
This is incorrect.
That would be cool, however it is not one of the existing or new functionality in Compass.

Q: What are some features and advantages of using the Aggregation Pipeline Builder versus writing the pipeline with the Mongo Shell?

Compass can translate the aggregation pipeline to the SQL equivalent.
This is incorrect.
It is not clear what would be the advantage of having such SQL statement, because the pipeline is designed with your document shape in mind.

Compass gives you a list of available stage names in a dropdown menu.
This is correct.

Compass informs you that a pipeline stage is correctly formed or has syntax errors.
This is correct.

With each stage, Compass displays examples of documents produced by the pipeline up to the given stage of the pipeline.
This is correct.

Compass can save pipelines to edit and use them later.
This is correct.

##########################################
# ODBC driver

https://www.microsoft.com/en-us/download/details.aspx?id=56029

Username: bic-user
Password: BicCool
Server: bic-sandbox-biconnector-gnkz6.mongodb.net
Port: 27015
Database: nba